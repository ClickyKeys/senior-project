---
title: "Analysis of Acute Care Hospital Quality in the United States"
author: "Lori Nichols"
output:
  html_document: default
---

<style type="text/css">
h1.title {
  color: DarkBlue;
  text-align: center;
}

h4.author {
  color: DarkBlue;
  text-align: center;
}
</style>

```{r setup, include = FALSE}
#
# The purpose of this program is to generate an HTML report containing the results of clustering and regression analyses. 
# The data used for this report is contained within the "data" folder downloaded from GitHub
# No changes to the code need to be made UNLESS 'data-cleanup.R' was run using new data apart from the files downloaded from GitHub - More details in the following code blocks
#

knitr::opts_chunk$set(echo = TRUE)
```

```{r importPackages, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Uncomment and run to intall the following packages (whichever haven't already been installed) if running this code for the first time

# install.packages("tidyverse")
# install.packages("rlist")
# install.packages("skimr")
# install.packages("caret") 
# install.packages("MASS")
# install.packages("cvms")
# install.packages("kableExtra")
# install.packages("VIM")
# install.packages("sjPlot") 
# install.packages("factoextra")
# install.packages("NbClust")
# install.packages("cluster")
# install.packages("rgl")
# install.packages("fossil")

# Uncomment and update the packages to avoid any loading issues
# update.packages("tidyverse")
# update.packages("rlist")
# update.packages("skimr")
# update.packages("caret") 
# update.packages("MASS")
# update.packages("cvms")
# update.packages("kableExtra")
# update.packages("VIM")
# update.packages("sjPlot") 
# update.packages("factoextra")
# update.packages("NbClust")
# update.packages("cluster")
# update.packages("rgl")
# update.packages("fossil")

# Load the necessary packages
library(tidyverse)
library(rlist)
library(skimr)
library(caret)
library(MASS)
library(cvms)
library(kableExtra)
library(VIM)
library(sjPlot)
library(factoextra)
library(NbClust)
library(cluster)
library(rgl)
library(fossil)
```

```{r readInitial, echo = FALSE, message = FALSE, warning = FALSE, error = FALSE}
# Read in the dataframe created using the 'data-cleanup.R' source code
# IMPORTANT: Uncomment and run if 'data-cleanup.R' was updated and run with new data apart from the files downloaded from Box

# cluster <- read_csv("data/Measures_by_Hospital_Acute_Care_120521.csv")
```

\

![](data/CMS-logo.jpg)

# Introduction

Quality of services provided at acute care hospitals around the United States is critical for improving individual health among patients admitted to hospitals for acute needs.  Measuring and monitoring this quality is an essential function carried out by the Centers for Medicare and Medicare Services (CMS) within the US department of health and human services.  CMS makes this quality assessment data publicly available online on an annual basis.  The goal of making the data publicly available is to help patients choose better hospitals for their health needs and to help data scientists identify target areas for quality improvement.  Studying patterns and variations in quality of services at acute care hospitals can help identify key insights for improving the healthcare system in the US.  Therefore, the goal of this project is to leverage CMS hospital quality data to:

* Identify groups of hospitals based on quality performance metrics; 
* Predict hospital quality using a variety of hospital characteristics;
* Create an interactive application to help patients identify the best hospital and understand quality of acute care areas in their county of residence.

\

# Data Source

The data obtained from CMS provide comprehensive information on quality of services provided at each hospital, as measured by several quality measures.  These quality measures assess four different domains of hospital quality, namely:

* Safety
* Person and Community Engagement
* Clinical Outcomes
* Efficiency and Cost Reduction

Over 80 quality measures are aggregated by CMS into appropriate domain scores and then used to assign a star rating to each acute care hospital on a scale of 1 through 5. For example, the quality measure, 'percentage of patients dying within 30 days after a heart attack' would fall under the Clinical Outcomes domain, whereas the quality measure 'percentage of patients reporting satisfaction with their care' would fall under the Person and Community Engagement domain. Because of the variation in the definitions and estimations of each quality measure and certain requirements, each hospital may only be eligible to report some of these measures.  For the purpose of this project, quality measure data from 29 different CMS data sets were collected and examined for inclusion in the following analyses.  Measures that were found to have > 25% missing data for all observations were eliminated from this analysis in order to limit the analysis to measures captured by most acute care hospitals in the US. After eliminating measures with too much missing data, a total of 64 measures were identified in 3,247 acute care hospitals distributed across the US and used for the following analysis.


\

# Part 1 - Exploratory Clustering Analysis of Hospital Quality Measures

The aim of this analysis is to explore how the Centers for Medicare and Medicaid Services (CMS) uses quality measure data to determine overall hospital quality. To achieve this, K-means clustering will be used.  K-means clustering partitions observations into a pre-defined number of groups (or clusters) based on similarities between variables.  Using this method, we can identify natural groupings in hospitals based on a variety of quality measures.  The resulting clusters can then be compared to CMS's overall star ratings to determine if they are useful for determining hospital quality as indicated by CMS's star rating system. 

\

## Data Preparation Process

### Data Normalization

The dataset being used for this analysis contains numerous variables, which are measured and scaled differently.  The clustering method being used uses k-means, which is a distance-based algorithm.  It is important to bring all variables to the same scale, otherwise variables measured in large valued units will dominate the computed dissimilarity and vars measured in small valued units will contribute very little. 

The normalization method being used is min-max normalization (or min-max scaling), which normalizes the values on a scale of 0 - 1, where 0 is the min value and 1 is the max, and all other data points fall between those two values./

```{r normalizeData, echo = TRUE, warning = FALSE, error = FALSE, message = FALSE}
# Normalize the data stored in the 'cluster' data frame
# IMPORTANT: uncomment and run if new data was read in from code block 'readInitial'

# normalize <- function(x) {
#   return ((x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE)))
# }
# 
# for(i in 2:length(cluster)) {
#   cluster[i] <- normalize(cluster[i])
# }
```


### K Nearest Neighbors Missing Value Imputation

The aggregated dataset containing quality measures being used for this analysis has missing values, which need to be removed or imputed prior to running the k-means algorithm.  The imputation method used is imputation with kNN, which looks at values that are close in space and imputes the mean value of the k nearest neighbors, where k is the sqare root of the total number of observations.

```{r imputeMissing, echo = TRUE, warning = FALSE, error = FALSE, message = FALSE}
# Impute missing values in the 'cluster' dataframe
# IMPORTANT: uncomment and run if new data was read in from code block 'readInitial' and normalized in code block 'normalizeData'

# kVal = floor(sqrt(nrow(cluster)))
# cluster_NAR <- cluster %>%
#   kNN(variable = 2:length(cluster), k = kVal, imp_var = FALSE)
```

```{r write, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Write the normalize and imputed dataframe to the 'data' folder so the 'readInitial', 'normalizeData', and 'imputeMissing' code blocks don't need to be run again for current data
# IMPORTANT: uncomment and run if new data was read in, normalize, and imputed in the above code blocks

# write_csv(cluster_NAR, "data/cluster_normalized_120521.csv")
```

```{r seed, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Set seed so the results are reproducible
set.seed(123)
```

\

## K Means Clustering Analysis

Of the 64 variables being used for this analysis, 48 were individual quality measure scores and 16 were summary scores estimated by CMS.  In order to best leverage these two types of variables, the clustering was repeated 3 different times and the results were compared.  This iterative approach to clustering can also be beneficial, because k-means clustering is an unsupervised, exploratory data analysis technique. The variables used for each of the 3 iterations of the clustering algorithm are provided below. 

* Quality measure scores only (48 variables)
* Summary scores only (16 variables)
* Both quality measure scores and summary scores (64 variables)


```{r readData_All, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Read in the current dataframe containing normalized and imputed variables
cluster_norm <- read_csv("data/cluster_normalized_120521.csv")

# Remove facility ID from the dataframe
cluster_norm_vars <- cluster_norm %>%
  dplyr::select(c(-"Facility.ID"))
```

```{r readData_MeasureScores, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Select all of the measure scores only
cluster_norm_vars_raw <- cluster_norm %>%
  dplyr::select(c(-"Facility.ID", -"Total.Performance.Score", -"Weighted.Efficiency.and.Cost.Reduction.Domain.Score",
                  -"Overall.Rating.of.Hospital.Dimension.Score", -"Unweighted.Normalized.Efficiency.and.Cost.Reduction.Domain.Score",
                  -"Weighted.Person.and.Community.Engagement.Domain.Score", -"Unweighted.Person.and.Community.Engagement.Domain.Score",
                  -"Weighted.Normalized.Clinical.Outcomes.Domain.Score", -"HCAHPS.Consistency.Score", -"HCAHPS.Base.Score", -"Discharge.Information.Dimension.Score",
                  -"Cleanliness.and.Quietness.of.Hospital.Environment.Dimension.Score", -"Communication.About.Medicines.Dimension.Score", -"Care.Transition.Dimension.Score",
                  -"Responsiveness.of.Hospital.Staff.Dimension.Score", -"Communication.with.Doctors.Dimension.Score", -"Communication.with.Nurses.Dimension.Score",
                  -"H_STAR_RATING.x"))
```

```{r readData_SummaryScores, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Select all of the summary scores only
cluster_norm_vars_summary <- cluster_norm %>%
  dplyr::select(c(-"Facility.ID")) %>%
  dplyr::select(c("Total.Performance.Score", "Overall.Rating.of.Hospital.Dimension.Score", "Weighted.Efficiency.and.Cost.Reduction.Domain.Score",
                  "Unweighted.Normalized.Efficiency.and.Cost.Reduction.Domain.Score", "Weighted.Person.and.Community.Engagement.Domain.Score",
                  "Unweighted.Person.and.Community.Engagement.Domain.Score", "Weighted.Normalized.Clinical.Outcomes.Domain.Score", "HCAHPS.Consistency.Score",
                  "HCAHPS.Base.Score", "Discharge.Information.Dimension.Score", "Cleanliness.and.Quietness.of.Hospital.Environment.Dimension.Score",
                  "Communication.About.Medicines.Dimension.Score", "Care.Transition.Dimension.Score", "Responsiveness.of.Hospital.Staff.Dimension.Score",
                  "Communication.with.Doctors.Dimension.Score", "Communication.with.Nurses.Dimension.Score", "H_STAR_RATING.x"))
```

\

### Calculating the Rand Index

To determine similarity between hospital clusters obtained from k means clustering and CMS star ratings (rated from 1 - 5), the Rand Index algorithm was used. RI is a similarity computation method which compares two different classifications of observations.  For the purpose of this analysis, one classification will be the results of the k-means clustering algorithm, and the other classification will be the overall star rating (1-5) determined by CMS.  The rand index algorithm outputs a value between 0 and 1, where 0 indicates no similarity between the two classifications and 1 indicates perfect similarity between them.  The output is adjusted (adjusted RI) to account for random chance groupings of observations.  

Because k means clustering can produce variable number of clusters, the adjusted RI was calculated for 2 - 8 clusters. The results of the adjusted RI algorithm for each cluster was visualized on a bar chart.  This analysis was then repeated for each iteration of the clustering analysis described above. 

*Results:* For all 3 clustering iterations, the adjusted RI indicates low similarity between the clustering results and CMS star rating.  These results were consistently low regardless of the number of clusters extracted. However, there is a significant increase in the adjusted RI when the number of clusters was > 6.  Interestingly, comparing star ratings to the iteration using only summary scores resulted in the lowest adjusted RI for all k clusters. 

*Conclusion:* This is likely because the clustering algorithm is significantly different from the methodology CMS uses to determine overall star rating. Nevertheless, this clustering approach can help identify groups of hospitals that exhibit similar characteristics. 

```{r RandIndex_Function, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Function to calculate the adjusted rand index given the calculated clusters and the actual star ratings
# Given a number of clusters 'x' and a dataframe 'y'
RI <- function(x, y) {
  
  # Create the clusters
  km4 <- kmeans(y, centers = x, nstart = 55)
  
  # Add cluster results to original data frame
  cluster_results <- as.data.frame(km4$cluster)
  hosp_clusters <- bind_cols(cluster_norm, cluster_results)
  
  # Select only the 'Facility ID' and the cluster results (cluster each facility belongs to)
  hosp_clusters <- hosp_clusters %>%
    dplyr::select(c("Facility.ID", "km4$cluster")) %>%
    rename(cluster = `km4$cluster`) %>%
    transform(Facility.ID = as.character(Facility.ID))
    
  # Read in the CMS dataset 'Hospital_General_Information.csv'
  star_ratings <- read_csv("data/Hospital_General_Information.csv")
  
  # Select only the 'Facility ID' and the 'Hospital overall rating' (star rating)
  star_ratings <- star_ratings %>%
    filter(`Hospital Type` == "Acute Care Hospitals") %>%
    dplyr::select(c("Facility ID", "Hospital overall rating")) %>%
    rename(Facility.ID = `Facility ID`) %>%
    rename(Star.Rating = `Hospital overall rating`) %>%
    transform(Star.Rating = as.numeric(Star.Rating))
    
  star_ratings$Facility.ID <- star_ratings$Facility.ID %>%
    trimws("left", "0")
    
  # Join the clustering results and the actual star rating into a dataset
  hosp_cluster_comparison <- full_join(hosp_clusters, star_ratings)
    
  # clust_analysis <- hosp_cluster_comparison %>%
  #   group_by(cluster, Star.Rating) %>%
  #   count()
  #   
  # clust_analysis %>%
  #   ggplot() +
  #   geom_bar(mapping = aes(x = Star.Rating, y = n, fill = cluster), stat = "identity") +
  #   facet_wrap(~ cluster)
    
  # Remove any hospitals with missing data
  hosp_clust_comp <- hosp_cluster_comparison %>%
    na.omit()
    
  # Use the adjusted rand index to compare the clustering results to the actual star rating
  adj.rand.index(hosp_clust_comp$Star.Rating,  hosp_clust_comp$cluster)
}
```


```{r k-means_Measures_RandIndex2, echo = TRUE, warning = FALSE, error = FALSE, message = FALSE}
# For measure scores only, uses the function in code block 'RandIndex_Function' to plot the adjusted rand index for a various number of clusters
# Create list to hold the number of clusters ('kVal') and the adjusted rand index
kVal <- NULL
randIndex <- NULL

# For 2 - 8 clusters, calculate the rand index and store the results to a dataframe
for(i in 2:8) {
  kVal <- list.append(kVal, i)
  randIndex <- list.append(randIndex, RI(i, cluster_norm_vars_raw))
}

# Transform the dataframe into a tibble
randIndexResults <- tibble(kVal = as.character(kVal), randIndex)

# Plot the results on a bar chart
randIndexResults %>%
  ggplot() +
  geom_bar(mapping = aes(x = kVal, y = randIndex, fill = kVal), stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "RdPu", aesthetics = "fill") +
  labs(title = "Adjusted RI for Quality Meausures Only")
```


```{r k-means_Summary_RandIndex3, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# For summary scores only, uses the function in code block 'RandIndex_Function' to plot the adjusted rand index for a various number of clusters
# Create list to hold the number of clusters ('kVal') and the adjusted rand index
kVal <- NULL
randIndex <- NULL

# For 2 - 8 clusters, calculate the rand index and store the results to a dataframe
for(i in 2:8) {
  kVal <- list.append(kVal, i)
  randIndex <- list.append(randIndex, RI(i, cluster_norm_vars_summary))
}

# Transform the dataframe into a tibble
randIndexResults <- tibble(kVal = as.character(kVal), randIndex)

# Plot the results on a bar chart
randIndexResults %>%
  ggplot() +
  geom_bar(mapping = aes(x = kVal, y = randIndex, fill = kVal), stat = "identity", position = "dodge") +
  scale_fill_brewer(palette = "RdPu", aesthetics = "fill") +
  labs(title = "Adjusted RI for Summary Scores Only")

```


```{r k-means_All_RandIndex1, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# For all measure and summary scores, uses the function in code block 'RandIndex_Function' to plot the adjusted rand index for a various number of clusters
# Create list to hold the number of clusters ('kVal') and the adjusted rand index
kVal <- NULL
randIndex <- NULL

# For 2 - 8 clusters, calculate the rand index and store the results to a dataframe
for(i in 2:8) {
  kVal <- list.append(kVal, i)
  randIndex <- list.append(randIndex, RI(i, cluster_norm_vars))
}

# Transform the dataframe into a tibble
randIndexResults <- tibble(kVal = as.character(kVal), randIndex)

# Plot the results on a bar chart
randIndexResults %>%
  ggplot() +
  geom_bar(mapping = aes(x = kVal, y = randIndex, fill = kVal), 
           stat = "identity", 
           position = "dodge") +
  scale_fill_brewer(palette = "RdPu", aesthetics = "fill") +
  labs(title = "Adjusted RI for All Variables")
```

\

### K-means Clustering Visualizations

The K-means clusters used in the adjusted RI index are visualized below on a scatterplot.  The results shown are for 4 and 8 clusters within each iteration. This number of clusters was selected based on the adjusted RI results, which showed that similarity to overall star rating was consistently highest with 8 clusters and consistently low with 4 clusters.  Notably, the visualizations shown below are depicted on a two-dimensional plane where the x-axis and y-axis are dimensions generated through Principle Component Analysis (PCA) - a dimensionality reduction algorithm - that account for most of the variation in the original dataset.  Considering the original dataset has high dimensionality, there may be distinct clusters that are not visually distinct on this 2d projection.    

*Results:* For the clustering using all measures, there appears to be some significant clustering behavior, and the algorithm has done a fairly good job of capturing the clusters. However, a common issue with k-means clustering is apparent.  K-means assumes even, spherical clusters and therefore has difficulty capturing elongated clusters.  There appear to be multiple distinct, elongated clusters present that the algorithm has difficulty capturing when a low number of clusters is specified. /

```{r k-means_Plot_Clusters, fig.show = "hold", out.width = "50%", echo = TRUE, message = FALSE, warning = FALSE}
# Function to plot the clusters for a given number of clusters 'x', a dataframe 'y', and a title 'z'
clustering <- function(x, y, z) {
  km4 <- kmeans(y, centers = x, nstart = 55)
  fviz_cluster(km4, data = y, ellipse.type = c("norm"), 
               geom = "point", 
               ggtheme = theme_minimal()) +
    scale_fill_brewer(palette = "rainbow") +
    labs(title = z)
}

#For quality measure scores only
clustering(4, cluster_norm_vars_raw, "4 Clusters - Quality Measure Scores")
clustering(8, cluster_norm_vars_raw, "8 Clusters - Quality Measure Scores")

#For summary scores only
clustering(4, cluster_norm_vars_summary, "4 Clusters - Summary Scores")
clustering(8, cluster_norm_vars_summary, "4 Clusters - Summary Scores")

#For all measure and summary scores in data frame
clustering(4, cluster_norm_vars, "4 Clusters - All Scores")
clustering(8, cluster_norm_vars, "8 Clusters - All Scores")
```

\

# Part 2 - Predicting Hospital Quality Using Hospital Characteristic Data

The aim of this analysis is to determine if hospital star ratings can be predicted using several hospital characteristic-related variables.  Hospital characteristics data were also obtained from CMS, and include characteristics such as: availability of emergency services, hospital ownership type, and number of healthcare personnel employed at the hospital. 
Because overall star rating is an ordinal variable from 1 - 5, ordinal logistic regression was used to model the relationship between star ratings (the ordinal response variable) and hospital characteristics (the explanatory variables).  Before building the prediction model, the hospital dataset was split into a training dataset and a testing dataset using an 80/20 split.

The model was then developed using the training dataset and evaluated for accuracy using the test dataset.  The model was also tested for overfitting by evaluating accuracy using the training dataset itself.     

```{r readFiles, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Read in the CMS general hospital information file and provider of services files from CMS
# IMPORTANT: The following code may require significant changes if using updated datasets downloaded from the CMS website
star_ratings <- read_csv("data/Hospital_General_Information.csv")
hospital_characteristics <- read_csv("C:/Users/lkn56/Desktop/School Stuff/Fall 2021/CSCI 487/hospitals_current_data/CHARACTERISTICS/Provider_of_Services_File_Hospital_Non_Hospital_Facilities_Dataset_2020_Q4.csv")
```

```{r wrangleData, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE}
# Select 'Facility ID', 'Facility Name', and 'Emergency Services' and 'Hospital overall rating' (star rating) columns from the general hospital information dataset
hospitals <- star_ratings %>%
  filter(`Hospital Type` == "Acute Care Hospitals") %>%
  dplyr::select(c("Facility ID", "Facility Name", 
                  "Emergency Services", 
                  "Hospital overall rating")) %>%
  rename(Facility.ID = "Facility ID") %>%
  rename(star_rating = "Hospital overall rating") %>%
  mutate(star_rating = as.numeric(star_rating)) %>%
  na.omit()

# Select the desired characteristics from the provider of services dataset
# IMPORTANT: Variables causing multicollinearity and categorical data with a significantly uneven distribution have not been selected.  Selecting more variales may cause the ordinal regression model in code block 'regressionModel' to fail
characteristics <- hospital_characteristics %>%
  dplyr::select(c("PRVDR_NUM", "CBSA_URBN_RRL_IND", "ACRDTN_TYPE_CD",
                  "CRTFD_BED_CNT", "MDCL_SCHL_AFLTN_CD", "OPRTG_ROOM_CNT", 
                  "CRNA_CNT", "LPN_LVN_CNT", "DIETN_CNT", "LAB_TCHNCN_CNT", 
                  "NRS_PRCTNR_CNT", "PHYSN_CNT", "REG_PHRMCST_CNT", 
                  "NUCLR_MDCN_TCHNCN_CNT", "TOT_AFLTD_ESRD_CNT", "TOT_AFLTD_HHA_CNT", 
                  "TOT_AFLTD_SNF_CNT", "GNRL_CNTL_TYPE_CD")) %>%
  rename(Facility.ID = "PRVDR_NUM") %>%
  mutate(ACRDTN_TYPE_CD = as.character(ACRDTN_TYPE_CD)) %>%
  mutate(MDCL_SCHL_AFLTN_CD = as.character(MDCL_SCHL_AFLTN_CD))

# Join the dataframes containing hospital information and characteristics into one dataframe
char_and_rate <- left_join(hospitals, characteristics, by = "Facility.ID")

# Recategorize - Some categorical variables contain many categories which can be recategorized accordingly to have a more even distribution of data
char_and_rate <- char_and_rate %>%
  mutate(star_rating = as.factor(star_rating)) %>%
  mutate(GNRL_CNTL_TYPE_CD = case_when(GNRL_CNTL_TYPE_CD == "01" ~ "Private_NFP", 
                                       GNRL_CNTL_TYPE_CD == "02" ~ "Private_NFP", 
                                       GNRL_CNTL_TYPE_CD == "05" ~ "Government", 
                                       GNRL_CNTL_TYPE_CD == "06" ~ "Government", 
                                       GNRL_CNTL_TYPE_CD == "07" ~ "Government", 
                                       GNRL_CNTL_TYPE_CD == "10" ~ "Government", 
                                       GNRL_CNTL_TYPE_CD == "03" ~ "Other", 
                                       GNRL_CNTL_TYPE_CD == "08" ~ "Other", 
                                       GNRL_CNTL_TYPE_CD == "09" ~ "Other", 
                                       TRUE ~ "Private_FP")) %>%
  mutate(ACRDTN_TYPE_CD = case_when(ACRDTN_TYPE_CD == "2" ~ "1", 
                                    ACRDTN_TYPE_CD == "3" ~ "1", 
                                    ACRDTN_TYPE_CD == "9" ~ "1", 
                                    TRUE ~ "0")) %>%
  mutate(MDCL_SCHL_AFLTN_CD = case_when(MDCL_SCHL_AFLTN_CD == "2" ~ "1", 
                                        MDCL_SCHL_AFLTN_CD == "3" ~ "1", 
                                        MDCL_SCHL_AFLTN_CD == "4" ~ "0", 
                                        TRUE ~ "1"))  %>%
  rename(Emergency_Services = `Emergency Services`, Accredited = ACRDTN_TYPE_CD,
         Urban_Rural_Indicator = CBSA_URBN_RRL_IND, Certified_Bed_Count = CRTFD_BED_CNT, 
         Med_School_Affiliated = MDCL_SCHL_AFLTN_CD, Operating_Room_Count = OPRTG_ROOM_CNT,
         CRNA_Count = CRNA_CNT, LPN_LVN_Count = LPN_LVN_CNT, Dietician_Count = DIETN_CNT,
         Lab_Tech_Count = LAB_TCHNCN_CNT, Nurse_Practitioner_Count = NRS_PRCTNR_CNT,
         Physician_Count = PHYSN_CNT, Registered_Pharmacist_Count = REG_PHRMCST_CNT,
         Nuclear_Med_Tech_Count = NUCLR_MDCN_TCHNCN_CNT, Total_Affiliated_ESRD_Count = TOT_AFLTD_ESRD_CNT,
         Total_Affiliated_HHC_Count = TOT_AFLTD_HHA_CNT, Total_Affiliated_SNF_Count = TOT_AFLTD_SNF_CNT,
         Ownership_Type = GNRL_CNTL_TYPE_CD)
```

```{r setSeed, echo = FALSE, warning = FALSE, erorr = FALSE}
# Set seed so the results are reproducible
set.seed(123)
```

```{r regressionData, echo = TRUE, warning = FALSE, error = FALSE, message = FALSE}
# Split the datafarme into a dataframe containing a random 80% of the data for training the model and a datarame containing the remaining 20% of data for testing the model
training_data <- char_and_rate %>%
  dplyr::select(c(-"Facility.ID", -"Facility Name")) %>%
  sample_frac(.80)

training_data_actual <- training_data %>%
  dplyr::select(c("star_rating"))

#testing data - predict on 20% of the data
testing_data <- anti_join(char_and_rate, training_data)

testing_data_actual <- testing_data %>%
  dplyr::select(c("star_rating"))

testing_data <- testing_data %>%
  dplyr::select(c(-"Facility.ID", -"star_rating", -"Facility Name"))
```

\

## Ordinal Regression Summary Table

The following table is a summary of the ordinal logistic regression model results.  These results show a coefficient (slope) for each predictor variable, a standard error of the coefficient, and an odds ratio, calculated as the exponent of the coefficient.  A t-value is also provided by the model, which was used to determine the p-value presented below.  Based on a p value < 0.05, the significant variables are: Certified bed count, Medical school affiliation(y/n), LPN/LVN count, Nuclear medicine technician count, and Ownership type.  The following are example interpretations of the coefficients of 'Certified bed count' and 'Ownership type':

* Certified bed count - For every unit increase in certified bed count, we expect the odds of a higher star rating to change by .999, given all other variables in the model are held constant. 
* Ownership type - When ownership type changes from 'Government' to 'Other', we expect the odds of a higher star rating to change by 2.0, given all other variables in the model are held constant. Additionally, when ownership changes from 'Government' to 'Priviate Not-for-profit', we expect the odds of a higher star rating to change by 2.77 given all other variables in the model are held constant.

```{r regressionModel, echo = TRUE, warning = FALSE, error = FALSE}
# Build the ordinal regression model using the training dataframe
model_fit <- polr(star_rating~., data = training_data, Hess = TRUE)

# Add some statistics to the ordinal regression model summary table to evaluate results and determine significant variables
summary_table <- coef(summary(model_fit))
pval <- pnorm(abs(summary_table[, "t value"]),lower.tail = FALSE)* 2
summary_table <- cbind(summary_table, "p value" = round(pval, 3))
summary_table <- as.data.frame(summary_table)

# Prepare the summary table for display
summary_table <- summary_table %>%
  mutate(Variables = rownames(summary_table)) %>%
  rename(Coefficients = Value) %>%
  mutate(Odds.ratio = exp(Coefficients)) %>%
  slice(1:20)

# Remove row names
rownames(summary_table) <- NULL

# Reorder table for display
summary_table <- subset(summary_table, select = c(5, 1, 6, 2, 3, 4)) 

# Display a formatted table
summary_table %>%
  kbl(digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), 
                full_width = T, 
                position = "center", 
                html_font = "Calibri") %>%
  column_spec(6, background = ifelse(summary_table$`p value`[1:20] < 0.05, 
                                  "lightgreen", 
                                  "white"))
```

\

## Confusion Matrix - Testing Accuracy of Prediction

The random 20% test dataset was used to evaluate the accuracy of the ordinal logistic regression model developed above. A prediction matrix was calculated using the "predict" function, which generated a matrix of predictions for each value of the ordinal variable (star ratings).  To test the accuracy of prediction, a confusion matrix was generated using the actual star ratings for each hospital as the actual class and the maximum predicted star rating from the prediction matrix as the predicted class. 

Result: The confusion matrix suggested an overall accuracy of 32.2%, and a balanced accuracy of 51.6%.  Accuracy of prediction was highest for 3 and 4 star hospitals.  
To test for possible overfitting, a re-prediction was performed on the 80% data used to train the regression model.  The confusion matrix results suggested an accuracy of 34.7% and a balanced accuracy of 53.7%.  This result indicates that there are no concerns of overfitting.

Conclusion: The overall and balanced prediction accuracy are low.  These variables are not sufficient to predict hospital quality with a high accuracy.  The model does show some ability to predict 3 & 4 star hospitals (as shown in the confusion matrix), and perhaps the model can be tuned further using variables from other sources.     

```{r testRegression, echo = FALSE, warning = FALSE, error = FALSE}
# Predict star ratings using the model built in code block 'regressionModel' and testing data
# Calculate a prediction matrix containing probabilities for each hospital for each star rating value
prediction_testing <- round(predict(model_fit, testing_data, type = "p"), 3)
prediction_testing <- as.data.frame(prediction_testing) 
  
# Determine the star rating with the highest probability for each hospital and store value to a new column
prediction_testing$prediction_testing_max <- pmax(prediction_testing$`1`, 
                                  prediction_testing$`2`, 
                                  prediction_testing$`3`, 
                                  prediction_testing$`4`, 
                                  prediction_testing$`5`)
prediction_testing$predicted_class <- NA

prediction_testing_class <- prediction_testing %>%
  mutate(predicted_class = as.factor(ifelse(prediction_testing_max == `4`, "4", 
                                   ifelse(prediction_testing_max == `3`, "3",
                                          ifelse(prediction_testing_max == `2`, "2",
                                                 ifelse(prediction_testing_max == `1`, "1", "5"))))))


prediction_testing_class$predicted_class <- factor(prediction_testing_class$predicted_class, 
                                           levels = c("1", "2", "3", "4", "5"))

# Predict star ratings using the model in code block 'regressionModel' and training data
# Calculate a prediction matrix containing probabilities for each hospital for each star rating value
prediction_training <- round(predict(model_fit, training_data, type = "p"), 3)
prediction_training <- as.data.frame(prediction_training) 
  
# Calculate the star rating with the highest probability for each hospital and store value to a new column
prediction_training$prediction_training_max <- pmax(prediction_training$`1`, 
                                  prediction_training$`2`, 
                                  prediction_training$`3`, 
                                  prediction_training$`4`, 
                                  prediction_training$`5`)
prediction_training$predicted_class <- NA

prediction_training_class <- prediction_training %>%
  mutate(predicted_class = as.factor(ifelse(prediction_training_max == `4`, "4", 
                                   ifelse(prediction_training_max == `3`, "3",
                                          ifelse(prediction_training_max == `2`, "2",
                                                 ifelse(prediction_training_max == `1`, "1", "5"))))))


prediction_training_class$predicted_class <- factor(prediction_training_class$predicted_class, 
                                           levels = c("1", "2", "3", "4", "5"))
```

```{r plotCM_Code, echo = TRUE, warning = FALSE, error = FALSE, fig.show = "hide"}
# Create a confusion matrix using the maximum predicted star rating and the actual star rating for each hospital
# IMPORTANT: This code block is just for showing the code in the HTML report
cm_testing <-  confusion_matrix(as.factor(testing_data_actual$star_rating), 
                        as.factor(prediction_testing_class$predicted_class))

plot_confusion_matrix(cm_testing$`Confusion Matrix`[[1]], 
                      place_x_axis_above = FALSE, 
                      palette = "Greens",
                      add_row_percentages = FALSE, 
                      add_col_percentages = FALSE) +
  labs(title = "Star Ratings prediction on testing data", 
       subtitle = "Overall Accuracy: 32.2%, Balanced Accuracy: 51.6%")
```

```{r plotCM_testing, echo = FALSE, warning = FALSE, error = FALSE, fig.show = "hold", out.width="100%"}
# For the predictions done on the testing data, create a confusion matrix using the maximum predicted star rating and the actual star rating for each hospital
cm_testing <-  confusion_matrix(as.factor(testing_data_actual$star_rating), 
                        as.factor(prediction_testing_class$predicted_class))

# Plot the confusion matrix results
plot_confusion_matrix(cm_testing$`Confusion Matrix`[[1]], 
                      place_x_axis_above = FALSE, 
                      palette = "Greens",
                      add_row_percentages = FALSE, 
                      add_col_percentages = FALSE) +
  labs(title = "Star Ratings prediction on testing data", 
       subtitle = "Overall Accuracy: 32.2%, Balanced Accuracy: 51.6%")
```


```{r plotCM_training, echo = FALSE, warning = FALSE, error = FALSE, fig.show = "hold", out.width="100%"}
# For predictions done on the training data, create a confusion matrix using the maximum predicted star rating and the actual star rating for each hospital
cm_training <- confusion_matrix(as.factor(training_data_actual$star_rating), 
                        as.factor(prediction_training_class$predicted_class))

# Plot the confusion matrix results
plot_confusion_matrix(cm_training$`Confusion Matrix`[[1]], 
                      place_x_axis_above = FALSE, 
                      palette = "Greens",
                      add_row_percentages = FALSE, 
                      add_col_percentages = FALSE) +
  labs(title = "Star Ratings prediction on training data", 
       subtitle = "Overall Accuracy: 34.7%, Balanced Accuracy: 53.3%")
```

\

# Part 3 - Interactive Tool to Identify the Best Hospital

Click the following link to use the Acute Care Hospital Finder app:

[Acute Care Hospital Finder](https://444-information-visualization.shinyapps.io/Acute_Care_Hospital_Finder/)

\

This interactive tool allows an individual user to select their state and county of choice (where data is available), generating the location of the best hospital in the selected county.  Additionally, the average quality domain scores for all hospitals in that county are presented in a separate tab (where available). 
